import OpenAI from 'openai';
import { MCPRequest } from '@/types/schedule';

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
const AVAILABLE_MODELS = [
  'gpt-4o-mini',      // ê°€ì¥ ì•ˆì •ì ì´ê³  ë¹ ë¦„
];

// OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const createOpenAIClient = (apiKey?: string) => {
  if (!apiKey) {
    throw new Error('OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë‚˜ ì§ì ‘ ì…ë ¥ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.');
  }
  
  return new OpenAI({
    apiKey: apiKey,
    dangerouslyAllowBrowser: true, // ë¸Œë¼ìš°ì €ì—ì„œ ì‚¬ìš©í•  ê²½ìš°
  });
};

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸ ë° ì„ íƒ
const selectAvailableModel = async (openai: OpenAI): Promise<string> => {
  for (const model of AVAILABLE_MODELS) {
    try {
      // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ìœ¼ë¡œ ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
      await openai.chat.completions.create({
        model: model,
        messages: [{ role: 'user', content: 'test' }],
        max_tokens: 5,
      });
      console.log(`âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ${model}`);
      return model;
    } catch (error: any) {
      if (error.status === 403) {
        console.log(`âŒ ëª¨ë¸ ${model} ì ‘ê·¼ ë¶ˆê°€: ${error.message}`);
        continue;
      }
      // ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ëª¨ë¸ ì‹œë„
      continue;
    }
  }
  
  // ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
  console.warn('âš ï¸ ëª¨ë“  ëª¨ë¸ ì ‘ê·¼ ì‹¤íŒ¨, gpt-4o-mini ì‚¬ìš©');
  return 'gpt-4o-mini';
};

// ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜ (ì˜ë„ ì¶”ì¶œë§Œ)
const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ HEM íšŒì‚¬ì˜ ì¼ì • ê´€ë¦¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ğŸ¯ LLMì˜ ì—­í• :
LLMì€ ì˜¤ì§ "ì˜ë„"ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. ë‚ ì§œë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ë§ˆì„¸ìš”.

## ğŸ“ ì‘ë‹µ í˜•ì‹:
ì¼ì • ìƒì„± ìš”ì²­ ì‹œ:
{
  "type": "mcp_request",
  "tool": "create_schedule",
  "arguments": {
    "title": "ì¼ì • ì œëª©",
    "when": {
      "relativeWeek": "this",        // "this" | "next"
      "weekday": "ìˆ˜",               // "ì›”" | "í™”" | "ìˆ˜" | "ëª©" | "ê¸ˆ" | "í† " | "ì¼"
      "time": "14:00",              // 24ì‹œê°„ í˜•ì‹ (HH:mm)
      "durationMinutes": 60          // ë¶„ ë‹¨ìœ„
    }
  }
}

## ğŸ’¡ ì˜ˆì‹œ:
- "ë‹¤ìŒì£¼ ìˆ˜ìš”ì¼ ì˜¤í›„ 2ì‹œ" â†’ relativeWeek: "next", weekday: "ìˆ˜", time: "14:00"
- "ì´ë²ˆì£¼ ê¸ˆìš”ì¼ ì˜¤ì „ 9ì‹œ" â†’ relativeWeek: "this", weekday: "ê¸ˆ", time: "09:00"
- "ë‚´ì¼ ì˜¤í›„ 3ì‹œ" â†’ relativeWeek: "next", weekday: "ëª©", time: "15:00"`;

// LLMì„ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ë¥¼ ì²˜ë¦¬ (ì¼ë°˜ ì‘ë‹µ + MCP ê¸°ëŠ¥)
export const processUserInputWithLLM = async (userInput: string, apiKey?: string): Promise<{
  type: 'mcp_request' | 'general_response';
  tool?: string;
  arguments?: Record<string, unknown>;
  message?: string;
  reasoning?: string;
}> => {
  try {
    const openai = createOpenAIClient(apiKey);
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
    const selectedModel = await selectAvailableModel(openai);
    
    // í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const currentTime = new Date();
    const dynamicSystemPrompt = SYSTEM_PROMPT.replace(
      /\$\{new Date\(\)\.toLocaleString\('ko-KR', \{[\s\S]*?\}\)\}/g,
      currentTime.toLocaleString('ko-KR', { 
        timeZone: 'Asia/Seoul',
        year: 'numeric',
        month: 'long',
        day: 'numeric',
        weekday: 'long',
        hour: '2-digit',
        minute: '2-digit',
        second: '2-digit'
      })
    );
    
    const completion = await openai.chat.completions.create({
      model: selectedModel,
      messages: [
        {
          role: 'system',
          content: dynamicSystemPrompt
        },
        {
          role: 'user',
          content: `ì‚¬ìš©ì ìš”ì²­: "${userInput}"`
        }
      ],
      temperature: 0.1,
      max_tokens: 800,
    });

    const response = completion.choices[0]?.message?.content;
    if (!response) return { type: 'general_response', message: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' };

    console.log('ğŸ” LLM ì›ë³¸ ì‘ë‹µ:', response);
    console.log('ğŸ” LLM ëª¨ë¸:', selectedModel);
    console.log('ğŸ” LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:', dynamicSystemPrompt);

    // JSON ì‘ë‹µ íŒŒì‹±
    try {
      const parsed = JSON.parse(response);
      console.log('ğŸ” LLM íŒŒì‹±ëœ ì‘ë‹µ:', parsed);
      
      if (parsed.type === 'mcp_request' && parsed.tool && parsed.arguments) {
        return {
          type: 'mcp_request',
          tool: parsed.tool,
          arguments: parsed.arguments,
          reasoning: parsed.reasoning
        };
      } else if (parsed.type === 'general_response' && parsed.message) {
        return {
          type: 'general_response',
          message: parsed.message,
          reasoning: parsed.context
        };
      }
    } catch (parseError) {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬
      console.error('LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError);
      return {
        type: 'general_response',
        message: response // ì›ë³¸ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
      };
    }

    return { type: 'general_response', message: 'ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' };
  } catch (error) {
    console.error('LLM API í˜¸ì¶œ ì‹¤íŒ¨:', error);
    return { type: 'general_response', message: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
  }
};

// ê¸°ì¡´ í•¨ìˆ˜ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
export const parseNaturalLanguageWithLLM = async (userInput: string, apiKey?: string): Promise<MCPRequest | null> => {
  const result = await processUserInputWithLLM(userInput, apiKey);
  
  if (result.type === 'mcp_request' && result.tool && result.arguments) {
    return {
      tool: result.tool,
      arguments: result.arguments
    };
  }
  
  return null;
};

// ëŒ€í™”í˜• ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•œ LLM ì„œë¹„ìŠ¤
export const collectMissingInfoWithLLM = async (
  userInput: string, 
  previousRequest: Record<string, unknown>, 
  missingFields: string[], 
  apiKey?: string
): Promise<{
  updatedRequest: Record<string, unknown>;
  isComplete: boolean;
  missingFields: string[];
  message: string;
} | null> => {
  try {
    const openai = createOpenAIClient(apiKey);
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
    const selectedModel = await selectAvailableModel(openai);
    
    // í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ ë™ì  ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    const currentTime = new Date();
    const currentTimeString = currentTime.toLocaleString('ko-KR', { 
      timeZone: 'Asia/Seoul',
      year: 'numeric',
      month: 'long',
      day: 'numeric',
      weekday: 'long',
      hour: '2-digit',
      minute: '2-digit',
      second: '2-digit'
    });
    
    const contextPrompt = `
## â° í˜„ì¬ ì‹œê°„ ì •ë³´:
í˜„ì¬ ì‹œì : ${currentTimeString}

## ğŸ“… ë‚ ì§œ ê´€ë ¨ ê·œì¹™ (ë°˜ë“œì‹œ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°):
- "ì˜¤ëŠ˜" = ${currentTime.toISOString().split('T')[0]} (í˜„ì¬ ë‚ ì§œ)
- "ë‚´ì¼" = ${new Date(currentTime.getTime() + 24*60*60*1000).toISOString().split('T')[0]} (í˜„ì¬ ë‚ ì§œ + 1ì¼)
- "ë‹¤ìŒ ì£¼" = í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ë‹¤ìŒ ì£¼ (ì›”ìš”ì¼ë¶€í„° ì¼ìš”ì¼)
- "ì´ë²ˆ ì£¼" = í˜„ì¬ ë‚ ì§œê°€ ì†í•œ ì£¼ (ì›”ìš”ì¼ë¶€í„° ì¼ìš”ì¼)

## ğŸ• ì‹œê°„ ê´€ë ¨ ê·œì¹™:
- "ì˜¤ì „" = 00:00-11:59
- "ì˜¤í›„" = 12:00-23:59 (ì˜¤í›„ 2ì‹œ = 14:00)
- "ë°¤" = 18:00-23:59
- "ì•„ì¹¨" = 06:00-11:59
- "ì ì‹¬" = 12:00-13:59

## ğŸ“ ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ):
{
  "updatedRequest": {
    "title": "ì¼ì • ì œëª©",
    "when": {
      "relativeWeek": "this",        // "this" | "next"
      "weekday": "ìˆ˜",               // "ì›”" | "í™”" | "ìˆ˜" | "ëª©" | "ê¸ˆ" | "í† " | "ì¼"
      "time": "14:00",              // 24ì‹œê°„ í˜•ì‹ (HH:mm)
      "durationMinutes": 60          // ë¶„ ë‹¨ìœ„
    }
  },
  "isComplete": true/false,
  "missingFields": ["ë¶€ì¡±í•œ í•„ë“œ ëª©ë¡"],
  "message": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€"
}

## ğŸ” ë¶„ì„í•  ì •ë³´:
ì´ì „ ìš”ì²­: ${JSON.stringify(previousRequest)}
ë¶€ì¡±í•œ í•„ë“œ: ${missingFields.join(', ')}
ì‚¬ìš©ì ì…ë ¥: "${userInput}"

## ğŸ’¡ ì˜ˆì‹œ:
- "ë‹¤ìŒì£¼ ìˆ˜ìš”ì¼ ì˜¤í›„ 2ì‹œ" â†’ relativeWeek: "next", weekday: "ìˆ˜", time: "14:00"
- "ì´ë²ˆì£¼ ê¸ˆìš”ì¼ ì˜¤ì „ 9ì‹œ" â†’ relativeWeek: "this", weekday: "ê¸ˆ", time: "09:00"
- "ë‚´ì¼ ì˜¤í›„ 3ì‹œ" â†’ relativeWeek: "next", weekday: "ëª©", time: "15:00"

ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì´ì „ ì •ë³´ì™€ ê²°í•©í•˜ê³ , when ê°ì²´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.`;

    const completion = await openai.chat.completions.create({
      model: selectedModel,
      messages: [
        {
          role: 'system',
          content: 'ë‹¹ì‹ ì€ ì¼ì • ìƒì„±ì— í•„ìš”í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'
        },
        {
          role: 'user',
          content: contextPrompt
        }
      ],
      temperature: 0.1,
      max_tokens: 800,
    });

    const response = completion.choices[0]?.message?.content;
    if (!response) return null;

    try {
      const parsed = JSON.parse(response);
      return parsed;
    } catch (parseError) {
      console.error('LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError);
      return null;
    }
  } catch (error) {
    console.error('LLM API í˜¸ì¶œ ì‹¤íŒ¨:', error);
    return null;
  }
};

// LLMì„ ì‚¬ìš©í•œ ì¼ì • ì‚­ì œ í™•ì¸
export const confirmScheduleDeletionWithLLM = async (
  userInput: string,
  foundSchedules: Array<{ id: string; title: string; startDate: string }>,
  apiKey?: string
): Promise<{
  confirmed: boolean;
  selectedSchedule?: string;
  message: string;
}> => {
  try {
    const openai = createOpenAIClient(apiKey);
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
    const selectedModel = await selectAvailableModel(openai);
    
    // í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ ë™ì  ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    const currentTime = new Date();
    const currentTimeString = currentTime.toLocaleString('ko-KR', { 
      timeZone: 'Asia/Seoul',
      year: 'numeric',
      month: 'long',
      day: 'numeric',
      weekday: 'long',
      hour: '2-digit',
      minute: '2-digit',
      second: '2-digit'
    });
    
    const contextPrompt = `
## â° í˜„ì¬ ì‹œê°„ ì •ë³´:
í˜„ì¬ ì‹œì : ${currentTimeString}

ì°¾ì€ ì¼ì •ë“¤:
${foundSchedules.map(s => `- ID: ${s.id}, ì œëª©: ${s.title}, ë‚ ì§œ: ${s.startDate}`).join('\n')}

ì‚¬ìš©ì ì…ë ¥: "${userInput}"

ì‚¬ìš©ìê°€ ì–´ë–¤ ì¼ì •ì„ ì‚­ì œí•˜ë ¤ê³  í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ , ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ í™•ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.
í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ê´€ë ¨ í‘œí˜„ì„ ì •í™•í•˜ê²Œ ì´í•´í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{
  "confirmed": true/false,
  "selectedSchedule": "ì„ íƒëœì¼ì •IDë˜ëŠ”null",
  "message": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€"
}`;

    const completion = await openai.chat.completions.create({
      model: selectedModel,
      messages: [
        {
          role: 'system',
          content: 'ë‹¹ì‹ ì€ ì¼ì • ì‚­ì œë¥¼ í™•ì¸í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.'
        },
        {
          role: 'user',
          content: contextPrompt
        }
      ],
      temperature: 0.1,
      max_tokens: 500,
    });

    const response = completion.choices[0]?.message?.content;
    if (!response) return { confirmed: false, message: 'ì‚­ì œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };

    try {
      const parsed = JSON.parse(response);
      return {
        confirmed: parsed.confirmed || false,
        selectedSchedule: parsed.selectedSchedule,
        message: parsed.message
      };
    } catch (parseError) {
      console.error('LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError);
      return { confirmed: false, message: 'ì‚­ì œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  } catch (error) {
    console.error('LLM API í˜¸ì¶œ ì‹¤íŒ¨:', error);
    return { confirmed: false, message: 'ì‚­ì œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
  }
};
